/**
 * æ—¥å¿—å·¥å…· - é‡æ„ç‰ˆæœ¬
 * ç»Ÿä¸€çš„æ—¥å¿—ç®¡ç†ç³»ç»Ÿ
 */

import * as path from 'path';
import * as fs from 'fs';
import * as os from 'os';
import * as zlib from 'zlib';
import { promisify } from 'util';
import { execSync } from 'child_process';

const gzip = promisify(zlib.gzip);
const readdir = promisify(fs.readdir);

export enum LogLevel {
  DEBUG = 0,
  INFO = 1,
  WARN = 2,
  ERROR = 3,
  FATAL = 4
}

interface LogEntry {
  timestamp: Date;
  level: LogLevel;
  message: string;
  context?: string;
  data?: any;
  error?: Error;
}

interface LoggerConfig {
  level: LogLevel;
  enableConsole: boolean;
  enableFile: boolean;
  logDir?: string;
  maxFileSize: number;
  maxFiles: number;
  contextName?: string;
  // æ–°å¢é…ç½®
  maxRetentionDays: number;      // æ—¥å¿—ä¿ç•™å¤©æ•°ï¼ˆé»˜è®¤7å¤©ï¼‰
  enableAutoCleanup: boolean;    // å¯ç”¨è‡ªåŠ¨æ¸…ç†ï¼ˆé»˜è®¤trueï¼‰
  cleanupInterval: number;       // æ¸…ç†é—´éš”ï¼ˆé»˜è®¤1å°æ—¶ï¼‰
  enableCompression: boolean;    // å¯ç”¨å‹ç¼©ï¼ˆé»˜è®¤falseï¼‰
  flushInterval: number;         // flushé—´éš”ï¼ˆé»˜è®¤10ç§’ï¼‰
  flushBatchSize: number;        // æ‰¹é‡å¤§å°ï¼ˆé»˜è®¤100æ¡ï¼‰
  enableSmartFlush: boolean;     // æ™ºèƒ½flushï¼ˆé»˜è®¤trueï¼‰
}

export class Logger {
  private config: LoggerConfig;
  private logDir: string;
  private logBuffer: LogEntry[] = [];
  private lastFlushTime: Date = new Date();

  private static instance?: Logger;
  private static loggers = new Map<string, Logger>();
  private static sharedFlushTimer?: NodeJS.Timeout;
  private static sharedCleanupTimer?: NodeJS.Timeout;

  constructor(config: Partial<LoggerConfig> = {}) {
    // æ£€æµ‹æ˜¯å¦åœ¨Electronç¯å¢ƒ
    const isElectron = typeof process !== 'undefined' &&
                       process.versions &&
                       process.versions.electron;

    this.config = {
      level: LogLevel.INFO,
      enableConsole: !isElectron,     // Electronç¯å¢ƒä¸‹ç¦ç”¨consoleè¾“å‡º,é¿å…åŒé‡è®°å½•
      enableFile: true,
      maxFileSize: 10 * 1024 * 1024, // 10MB (åŸ5MB)
      maxFiles: 3,                    // 3ä¸ªè½®è½¬æ–‡ä»¶ (åŸ5ä¸ª)
      maxRetentionDays: 7,           // ä¿ç•™7å¤©
      enableAutoCleanup: true,
      cleanupInterval: 60 * 60 * 1000, // 1å°æ—¶
      enableCompression: false,       // é»˜è®¤å…³é—­å‹ç¼©ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
      flushInterval: 10000,           // 10ç§’ (åŸ5ç§’)
      flushBatchSize: 100,
      enableSmartFlush: true,
      ...config
    };

    this.logDir = this.config.logDir || this.getDefaultLogDir();
    this.ensureLogDirectory();

    // ä¸åœ¨æ„é€ å‡½æ•°ä¸­å¯åŠ¨timerï¼Œæ”¹ç”¨å…¨å±€å…±äº«timer
  }

  static getInstance(): Logger {
    if (!Logger.instance) {
      Logger.instance = new Logger();
      Logger.startSharedTimers(); // å¯åŠ¨å…¨å±€å…±äº«timer
    }
    return Logger.instance;
  }

  static getLogger(context: string): Logger {
    if (!Logger.loggers.has(context)) {
      Logger.loggers.set(context, new Logger({ contextName: context }));
      Logger.startSharedTimers(); // ç¡®ä¿timerå¯åŠ¨
    }
    return Logger.loggers.get(context)!;
  }

  /**
   * æ¸…ç†æ‰€æœ‰å†å²æ—¥å¿—æ–‡ä»¶ï¼ˆç¨‹åºå¯åŠ¨æ—¶è°ƒç”¨ï¼‰
   * åŒæ­¥æ‰§è¡Œï¼Œç¡®ä¿æ¸…ç†å®Œæˆåæ‰ç»§ç»­å¯åŠ¨
   *
   * ä¼šæ¸…ç†ä»¥ä¸‹å¯èƒ½çš„æ—¥å¿—ç›®å½•ï¼š
   * - Windows: %APPDATA%/employee-monitor/logs å’Œ %APPDATA%/employee-safety-client/logs
   * - macOS: ~/Library/Logs/employee-monitor å’Œ ~/Library/Logs/employee-safety-client
   * - Linux: ~/.local/share/employee-monitor/logs å’Œ ~/.local/share/employee-safety-client/logs
   */
  static cleanupAllLogs(): void {
    try {
      // è·å–å¯èƒ½çš„æ—¥å¿—ç›®å½•åˆ—è¡¨
      const possibleLogDirs: string[] = [];

      // æ ‡å‡†æ—¥å¿—ç›®å½•
      const standardLogDir = new Logger().logDir;
      possibleLogDirs.push(standardLogDir);

      // é¢å¤–æ£€æŸ¥ employee-safety-client ç›®å½•ï¼ˆå†å²é—ç•™ï¼‰
      try {
        const appDataDir = process.env.APPDATA ||
                          path.join(os.homedir(), process.platform === 'darwin' ? 'Library/Logs' : '.local/share');
        const legacyLogDir = path.join(appDataDir, 'employee-safety-client', 'logs');
        if (legacyLogDir !== standardLogDir) {
          possibleLogDirs.push(legacyLogDir);
        }
      } catch {
        // å¿½ç•¥é”™è¯¯
      }

      let totalDeletedCount = 0;

      // æ¸…ç†æ‰€æœ‰å¯èƒ½çš„æ—¥å¿—ç›®å½•
      for (const logDir of possibleLogDirs) {
        if (!fs.existsSync(logDir)) {
          continue; // ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡
        }

        try {
          const files = fs.readdirSync(logDir);
          let deletedCount = 0;

          for (const file of files) {
            // åˆ é™¤æ‰€æœ‰æ—¥å¿—æ–‡ä»¶ï¼š.log, .log.*, .gz
            if (file.endsWith('.log') || file.includes('.log.') || file.endsWith('.gz')) {
              try {
                fs.unlinkSync(path.join(logDir, file));
                deletedCount++;
              } catch (error) {
                // å¿½ç•¥åˆ é™¤å¤±è´¥çš„æ–‡ä»¶ï¼ˆå¯èƒ½è¢«å ç”¨ï¼‰
                console.warn(`[Logger] Failed to delete log file: ${file}`, error);
              }
            }
          }

          if (deletedCount > 0) {
            console.info(`[Logger] Cleaned up ${deletedCount} log file(s) in ${logDir}`);
            totalDeletedCount += deletedCount;
          }
        } catch (error) {
          console.warn(`[Logger] Failed to cleanup logs in ${logDir}:`, error);
        }
      }

      if (totalDeletedCount > 0) {
        console.info(`[Logger] Total cleaned up ${totalDeletedCount} log file(s) on startup`);
      }
    } catch (error) {
      console.warn('[Logger] Failed to cleanup logs on startup:', error);
    }
  }

  // å…¨å±€å…±äº«çš„å®šæ—¶å™¨
  private static startSharedTimers(): void {
    // å¯åŠ¨flush timer
    if (!Logger.sharedFlushTimer && Logger.instance) {
      Logger.sharedFlushTimer = setInterval(() => {
        // flushæ‰€æœ‰loggerå®ä¾‹
        Logger.instance?.smartFlush().catch(() => {});
        Logger.loggers.forEach(logger => {
          logger.smartFlush().catch(() => {});
        });
      }, Logger.instance.config.flushInterval);
    }

    // å¯åŠ¨cleanup timer
    if (!Logger.sharedCleanupTimer && Logger.instance && Logger.instance.config.enableAutoCleanup) {
      // å¯åŠ¨æ—¶ç«‹å³æ¸…ç†ä¸€æ¬¡
      Logger.instance.cleanupOldLogs().catch(() => {});

      Logger.sharedCleanupTimer = setInterval(() => {
        Logger.instance?.cleanupOldLogs().catch(() => {});
      }, Logger.instance.config.cleanupInterval);
    }
  }

  // å…¨å±€æ¸…ç†
  static destroyAll(): void {
    if (Logger.sharedFlushTimer) {
      clearInterval(Logger.sharedFlushTimer);
      Logger.sharedFlushTimer = undefined;
    }

    if (Logger.sharedCleanupTimer) {
      clearInterval(Logger.sharedCleanupTimer);
      Logger.sharedCleanupTimer = undefined;
    }

    Logger.instance?.destroy();
    Logger.loggers.forEach(logger => logger.destroy());
    Logger.loggers.clear();
  }

  debug(message: string, data?: any): void {
    this.log(LogLevel.DEBUG, message, data);
  }

  info(message: string, data?: any): void {
    this.log(LogLevel.INFO, message, data);
  }

  warn(message: string, data?: any): void {
    this.log(LogLevel.WARN, message, data);
  }

  error(message: string, error?: Error | any, data?: any): void {
    const errorObj = error instanceof Error ? error : new Error(String(error));
    this.log(LogLevel.ERROR, message, data, errorObj);
  }

  fatal(message: string, error?: Error | any, data?: any): void {
    const errorObj = error instanceof Error ? error : new Error(String(error));
    this.log(LogLevel.FATAL, message, data, errorObj);
  }

  private log(level: LogLevel, message: string, data?: any, error?: Error): void {
    if (level < this.config.level) {
      return;
    }

    const entry: LogEntry = {
      timestamp: new Date(),
      level,
      message,
      context: this.config.contextName,
      data: data ? this.sanitizeData(data) : undefined,
      error
    };

    // ç«‹å³è¾“å‡ºåˆ°æ§åˆ¶å°
    if (this.config.enableConsole) {
      this.writeToConsole(entry);
    }

    // ç¼“å†²æ–‡ä»¶è¾“å‡º
    if (this.config.enableFile) {
      this.logBuffer.push(entry);
    }
  }

  private writeToConsole(entry: LogEntry): void {
    const timestamp = entry.timestamp.toISOString().substring(11, 19);
    const levelName = LogLevel[entry.level].padEnd(5);
    const context = entry.context ? `[${entry.context}]` : '';
    const message = `${timestamp} ${levelName} ${context} ${entry.message}`;

    switch (entry.level) {
      case LogLevel.DEBUG:
        console.debug(message, entry.data || '');
        break;
      case LogLevel.INFO:
        console.info(message, entry.data || '');
        break;
      case LogLevel.WARN:
        console.warn(message, entry.data || '');
        break;
      case LogLevel.ERROR:
      case LogLevel.FATAL:
        console.error(message, entry.error?.stack || entry.error || entry.data || '');
        break;
    }
  }

  private async writeToFile(): Promise<void> {
    if (this.logBuffer.length === 0) {
      return;
    }

    try {
      // å†™å…¥å‰æ£€æŸ¥ç£ç›˜ç©ºé—´
      const diskSpace = await this.checkDiskSpace();
      const availableGB = diskSpace.available / (1024 ** 3);

      if (availableGB < 0.1) { // å°äº100MB
        await this.emergencyCleanup();
        return;
      }

      const entries = [...this.logBuffer];
      this.logBuffer = [];
      this.lastFlushTime = new Date();

      const logFile = path.join(this.logDir, 'app.log');
      const logLines = entries.map(entry => this.formatLogEntry(entry)).join('\n') + '\n';

      // æ£€æŸ¥æ–‡ä»¶å¤§å°å¹¶è½®è½¬
      await this.rotateLogIfNeeded(logFile);

      // è¿½åŠ æ—¥å¿—
      await fs.promises.appendFile(logFile, logLines);

    } catch (error) {
      console.error('[Logger] Failed to write log file:', error);
    }
  }

  private formatLogEntry(entry: LogEntry): string {
    const timestamp = entry.timestamp.toISOString();
    const level = LogLevel[entry.level];
    const context = entry.context ? ` [${entry.context}]` : '';
    let message = `${timestamp} ${level}${context} ${entry.message}`;

    if (entry.data) {
      message += ` | Data: ${JSON.stringify(entry.data)}`;
    }

    if (entry.error) {
      message += ` | Error: ${entry.error.message}`;
      if (entry.error.stack) {
        message += `\nStack: ${entry.error.stack}`;
      }
    }

    return message;
  }

  private async rotateLogIfNeeded(logFile: string): Promise<void> {
    try {
      const stats = await fs.promises.stat(logFile);

      if (stats.size >= this.config.maxFileSize) {
        // 1. åˆ é™¤æœ€è€çš„æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        const oldestFile = `${logFile}.${this.config.maxFiles}`;
        try {
          await fs.promises.unlink(oldestFile);
        } catch {
          // æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¿½ç•¥
        }

        // 2. å€’åºé‡å‘½åç°æœ‰è½®è½¬æ–‡ä»¶
        for (let i = this.config.maxFiles - 1; i >= 1; i--) {
          const oldFile = `${logFile}.${i}`;
          const newFile = `${logFile}.${i + 1}`;

          try {
            await fs.promises.access(oldFile);
            await fs.promises.rename(oldFile, newFile);
          } catch {
            // æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç»§ç»­
          }
        }

        // 3. é‡å‘½åå½“å‰æ—¥å¿—æ–‡ä»¶
        await fs.promises.rename(logFile, `${logFile}.1`);

        // 4. éªŒè¯è½®è½¬åçš„æ–‡ä»¶æ•°é‡
        await this.verifyRotatedFiles(logFile);

        // 5. å‹ç¼©æ—§æ—¥å¿—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if (this.config.enableCompression) {
          await this.compressOldLogs(logFile);
        }
      }
    } catch (error) {
      console.error('[Logger] Log rotation failed:', error);
    }
  }

  // éªŒè¯è½®è½¬æ–‡ä»¶æ•°é‡
  private async verifyRotatedFiles(logFile: string): Promise<void> {
    try {
      const files = await readdir(this.logDir);
      const baseName = path.basename(logFile);
      const rotatedFiles = files.filter(f =>
        f.startsWith(baseName) &&
        f !== baseName &&
        !f.endsWith('.gz')
      );

      if (rotatedFiles.length > this.config.maxFiles) {
        console.warn(`[Logger] Found ${rotatedFiles.length} rotated files, expected max ${this.config.maxFiles}`);

        // æ¸…ç†è¶…å‡ºæ•°é‡çš„æ–‡ä»¶
        const sorted = rotatedFiles.sort((a, b) => {
          const numA = parseInt(a.split('.').pop() || '0');
          const numB = parseInt(b.split('.').pop() || '0');
          return numB - numA; // é™åºï¼Œæœ€å¤§çš„åœ¨å‰
        });

        for (let i = this.config.maxFiles; i < sorted.length; i++) {
          await fs.promises.unlink(path.join(this.logDir, sorted[i]));
          console.info(`[Logger] Cleaned up excessive rotated file: ${sorted[i]}`);
        }
      }
    } catch (error) {
      console.warn('[Logger] Failed to verify rotated files:', error);
    }
  }

  private getDefaultLogDir(): string {
    let logDir: string;

    try {
      // å°è¯•ä½¿ç”¨åº”ç”¨ç¨‹åºæ•°æ®ç›®å½•
      const appDataDir = process.env.APPDATA || 
                        path.join(os.homedir(), process.platform === 'darwin' ? 'Library/Logs' : '.local/share');
      logDir = path.join(appDataDir, 'employee-monitor', 'logs');
    } catch {
      // åå¤‡é€‰é¡¹ï¼šå½“å‰ç›®å½•çš„logsæ–‡ä»¶å¤¹
      logDir = path.join(process.cwd(), 'logs');
    }

    return logDir;
  }

  private ensureLogDirectory(): void {
    try {
      if (!fs.existsSync(this.logDir)) {
        fs.mkdirSync(this.logDir, { recursive: true });
      }
    } catch (error) {
      console.warn('[Logger] Failed to create log directory:', error);
      this.config.enableFile = false;
    }
  }

  private sanitizeData(data: any): any {
    try {
      // æ¸…ç†æ•æ„Ÿæ•°æ®å’Œå¾ªç¯å¼•ç”¨
      return JSON.parse(JSON.stringify(data, (key, value) => {
        // æ¸…ç†å¯†ç ç›¸å…³å­—æ®µ
        if (typeof key === 'string' && /password|token|secret|key|auth/i.test(key)) {
          return '[REDACTED]';
        }
        return value;
      }));
    } catch {
      return '[UNSERIALIZABLE_DATA]';
    }
  }

  // æ™ºèƒ½flushç­–ç•¥
  private async smartFlush(): Promise<void> {
    if (!this.config.enableSmartFlush) {
      return this.writeToFile();
    }

    const bufferSize = this.logBuffer.length;
    const hasErrorLogs = this.logBuffer.some(e => e.level >= LogLevel.ERROR);

    // æ¡ä»¶1: ç¼“å†²åŒºè¾¾åˆ°æ‰¹é‡å¤§å°
    // æ¡ä»¶2: æœ‰ERRORçº§åˆ«æ—¥å¿—ï¼ˆç«‹å³å†™å…¥ï¼‰
    if (bufferSize >= this.config.flushBatchSize || hasErrorLogs) {
      await this.writeToFile();
    }
  }

  // ç«‹å³flushæ‰€æœ‰ç¼“å†²çš„æ—¥å¿—
  async flush(): Promise<void> {
    await this.writeToFile();
  }

  // åŸºäºæ—¶é—´çš„æ¸…ç†ç­–ç•¥
  private async cleanupOldLogs(): Promise<void> {
    try {
      const files = await readdir(this.logDir);
      const now = Date.now();
      const maxAge = this.config.maxRetentionDays * 24 * 60 * 60 * 1000;

      for (const file of files) {
        if (!file.endsWith('.log') && !file.includes('.log.')) {
          continue; // è·³è¿‡éæ—¥å¿—æ–‡ä»¶
        }

        const filePath = path.join(this.logDir, file);
        try {
          const stats = await fs.promises.stat(filePath);
          const age = now - stats.mtimeMs;

          if (age > maxAge) {
            await fs.promises.unlink(filePath);
            console.info(`[Logger] Cleaned up old log file: ${file} (age: ${Math.round(age / 1000 / 60 / 60 / 24)} days)`);
          }
        } catch (error) {
          console.warn(`[Logger] Failed to cleanup ${file}:`, error);
        }
      }
    } catch (error) {
      console.error('[Logger] Cleanup task failed:', error);
    }
  }

  // æ£€æŸ¥ç£ç›˜ç©ºé—´
  private async checkDiskSpace(): Promise<{ available: number; total: number }> {
    try {
      if (process.platform === 'win32') {
        // Windows: ä½¿ç”¨wmicå‘½ä»¤
        const drive = this.logDir.charAt(0);
        const output = execSync(`wmic logicaldisk where "DeviceID='${drive}:'" get FreeSpace,Size`, { timeout: 5000 }).toString();
        const lines = output.trim().split('\n');
        if (lines.length > 1) {
          const parts = lines[1].trim().split(/\s+/);
          if (parts.length >= 2) {
            return { available: parseInt(parts[0]), total: parseInt(parts[1]) };
          }
        }
      } else {
        // macOS/Linux: ä½¿ç”¨dfå‘½ä»¤
        const output = execSync(`df -k "${this.logDir}"`, { timeout: 5000 }).toString();
        const lines = output.trim().split('\n');
        if (lines.length > 1) {
          const parts = lines[1].trim().split(/\s+/);
          return {
            available: parseInt(parts[3]) * 1024,
            total: parseInt(parts[1]) * 1024
          };
        }
      }
    } catch (error) {
      console.warn('[Logger] Failed to check disk space:', error);
    }

    return { available: 0, total: 0 };
  }

  // ç´§æ€¥æ¸…ç†
  private async emergencyCleanup(): Promise<void> {
    console.warn('[Logger] ğŸš¨ Emergency cleanup triggered!');

    try {
      // 1. åœæ­¢å†™å…¥
      const wasEnabled = this.config.enableFile;
      this.config.enableFile = false;

      // 2. æ¸…ç†æ‰€æœ‰è½®è½¬æ–‡ä»¶
      const files = await readdir(this.logDir);
      for (const file of files) {
        if (file.includes('.log.') || file.endsWith('.gz')) {
          try {
            await fs.promises.unlink(path.join(this.logDir, file));
            console.info(`[Logger] Emergency deleted: ${file}`);
          } catch {
            // å¿½ç•¥åˆ é™¤å¤±è´¥
          }
        }
      }

      // 3. å‹ç¼©å½“å‰æ—¥å¿—
      const currentLog = path.join(this.logDir, 'app.log');
      if (fs.existsSync(currentLog)) {
        const content = await fs.promises.readFile(currentLog, 'utf-8');
        const lines = content.split('\n');
        // åªä¿ç•™æœ€å1000è¡Œ
        const truncated = lines.slice(-1000).join('\n');
        await fs.promises.writeFile(currentLog, truncated);
        console.info(`[Logger] Truncated current log to last 1000 lines`);
      }

      // 4. æ¢å¤å†™å…¥
      this.config.enableFile = wasEnabled;
      console.info('[Logger] âœ… Emergency cleanup completed');
    } catch (error) {
      console.error('[Logger] Emergency cleanup failed:', error);
    }
  }

  // å‹ç¼©æ—§æ—¥å¿—æ–‡ä»¶
  private async compressOldLogs(logFile: string): Promise<void> {
    try {
      // å‹ç¼©ä».2å¼€å§‹çš„æ—§æ—¥å¿—ï¼Œä¿ç•™æœ€æ–°çš„.1ä¸å‹ç¼©
      for (let i = 2; i <= this.config.maxFiles; i++) {
        const oldFile = `${logFile}.${i}`;
        if (fs.existsSync(oldFile) && !oldFile.endsWith('.gz')) {
          await this.compressLogFile(oldFile);
        }
      }
    } catch (error) {
      console.warn('[Logger] Failed to compress old logs:', error);
    }
  }

  // å‹ç¼©å•ä¸ªæ—¥å¿—æ–‡ä»¶
  private async compressLogFile(filePath: string): Promise<void> {
    try {
      const content = await fs.promises.readFile(filePath);
      const compressed = await gzip(content);

      const compressedPath = `${filePath}.gz`;
      await fs.promises.writeFile(compressedPath, compressed);
      await fs.promises.unlink(filePath); // åˆ é™¤åŸæ–‡ä»¶

      const savedPercent = Math.round((1 - compressed.length / content.length) * 100);
      console.info(`[Logger] Compressed ${path.basename(filePath)} (saved ${savedPercent}%)`);
    } catch (error) {
      console.error(`[Logger] Failed to compress ${filePath}:`, error);
    }
  }

  // æ›´æ”¹æ—¥å¿—çº§åˆ«
  setLevel(level: LogLevel): void {
    this.config.level = level;
  }

  // æ¸…ç†èµ„æº
  destroy(): void {
    // ç«‹å³å†™å…¥æ‰€æœ‰ç¼“å†²çš„æ—¥å¿—
    this.writeToFile().catch(() => {
      // å¿½ç•¥é”™è¯¯
    });
  }

  // è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯
  getStats(): {
    logDir: string;
    bufferSize: number;
    config: LoggerConfig;
  } {
    return {
      logDir: this.logDir,
      bufferSize: this.logBuffer.length,
      config: { ...this.config }
    };
  }
}

// å¯¼å‡ºé»˜è®¤å®ä¾‹å’Œä¾¿æ·å‡½æ•°
export const logger = Logger.getInstance();

export const createLogger = (context: string): Logger => {
  return Logger.getLogger(context);
};

export default logger;